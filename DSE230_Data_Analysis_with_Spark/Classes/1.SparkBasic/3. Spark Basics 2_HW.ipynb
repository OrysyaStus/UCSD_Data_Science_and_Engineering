{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d6c2e7d9d384c814c2e8c3a6eeaa8168",
     "grade": false,
     "grade_id": "1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Setup Notebook for Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5dabb894218c50033ddf4893b4611119",
     "grade": false,
     "grade_id": "2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "##### <span style=\"color:red\">IMPORTANT: Only modify cells which have the following comment:</span>\n",
    "```python\n",
    "# Modify this cell\n",
    "```\n",
    "##### <span style=\"color:red\">Do not add any new cells when you submit the homework</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0bd6008f85b60d5be9760ff4d4148185",
     "grade": false,
     "grade_id": "3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08c1c197f80a25e078ed7178f5519042",
     "grade": false,
     "grade_id": "4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext(master=\"local[4]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "36bcf69a50f88a47298f4e92e57d245a",
     "grade": false,
     "grade_id": "5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "pickleFile=\"Tester/SparkBasics2.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26b692ca466ba9836dc74cca5398da2a",
     "grade": false,
     "grade_id": "6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Importing all packages necessary to complete the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c74678d07c9675080c6000afeeb1866e",
     "grade": false,
     "grade_id": "7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07f18ac1ef76f56d7bcd1089001e0048",
     "grade": false,
     "grade_id": "8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Write a function called **maxSum** which finds the maximum of each list and sums them all together. \n",
    "\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "listRDD=sc.parallelize([[3,4],[2,1],[7,9]]) \n",
    "print maxSum(listRDD)\n",
    "```\n",
    "\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "15 \n",
    "\n",
    "    Note: 15 = 4 + 2 + 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def maxSum(A):\n",
    "    RDD = A.map(lambda x: max(x))\\\n",
    "        .reduce(lambda x,y: x+y)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f391a474a9a1971583b169ab431b8702",
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[3, 4], [2, 1], [7, 9]]\n",
      "Correct Output: 15\n",
      "Great Job!\n",
      "\n",
      "Input: [[3, 42, 12, 4], [6, 0, -1], [32, 31, 2, 52, 3]]\n",
      "Correct Output: 100\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise1(pickleFile, maxSum,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ac7e5faa01941fa0b7f996bd5a6d93ac",
     "grade": false,
     "grade_id": "9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Write a function called **positiveCos** which uses **filter** to output elements whose cosine is positive. \n",
    "\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "    RDD=sc.parallelize([0,2,1])\n",
    "    print positiveCos(RDD)\n",
    "    print positiveCos(RDD).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "[0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def positiveCos(A):\n",
    "    RDD = A.map(lambda x: (x, np.cos(x)))\\\n",
    "        .filter(lambda (k,v): v >0)\\\n",
    "        .map(lambda (k,v): k)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c6b10a2f4792f04a4df88b7ebf267b14",
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 1, 2]\n",
      "Correct Output: [0, 1]\n",
      "Great Job!\n",
      "\n",
      "Input: [4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Correct Output: [5, 6, 7, 11]\n",
      "Great Job!\n",
      "\n",
      "Input: [-4, -3, -2, -1]\n",
      "Correct Output: [-1]\n",
      "Great Job!\n",
      "\n",
      "Input: [0, 2, 1]\n",
      "Correct Output: [0, 1]\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise2(pickleFile, positiveCos,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "210866650f338f6ab040312e58967736",
     "grade": false,
     "grade_id": "10",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 3\n",
    "Write a function called **longWords** which uses **filter** to output all words whose length is greater than or equal to 4. \n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "wordRDD=sc.parallelize(['this','is','the','best','mac','ever'])\n",
    "print longWords(RDD)\n",
    "print longWords(RDD).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "['this', 'best', 'ever']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def longWords(A):\n",
    "    RDD = A.filter(lambda x: len(x) >= 4)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "419256bb497582efbe78c298ec93bf87",
     "grade": true,
     "grade_id": "ex_3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['this', 'is', 'the', 'best', 'mac', 'ever']\n",
      "Correct Output: ['this', 'best', 'ever']\n",
      "Great Job!\n",
      "\n",
      "Input: ['Hasta', 'la', 'vista', 'baby']\n",
      "Correct Output: ['Hasta', 'vista', 'baby']\n",
      "Great Job!\n",
      "\n",
      "Input: ['A', 'long', 'time', 'ago', 'in', 'a', 'galaxy', 'far', 'far', 'away']\n",
      "Correct Output: ['long', 'time', 'galaxy', 'away']\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise3(pickleFile, longWords,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "810fef2db1767467df3491deff531bed",
     "grade": false,
     "grade_id": "11",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 4\n",
    "\n",
    "Write a function **collectRange** which uses **flatMap** to collect all the elements from 1 to x for each element x in the given listRDD. \n",
    "\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "RDD=sc.parallelize([2,3,5])\n",
    "print collectRange(RDD)\n",
    "print collectRange(RDD).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "[1, 2, 1, 2, 3, 1, 2, 3, 4, 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def collectRange(A):\n",
    "    RDD = A.flatMap(lambda x: (range(1, x+1)))\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97823af1e6ac50b6675c5d7f01ee3eef",
     "grade": true,
     "grade_id": "ex_4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [2, 3, 5]\n",
      "Correct Output: [1, 2, 1, 2, 3, 1, 2, 3, 4, 5]\n",
      "Great Job!\n",
      "\n",
      "Input: [5, 3, 1]\n",
      "Correct Output: [1, 2, 3, 4, 5, 1, 2, 3, 1]\n",
      "Great Job!\n",
      "\n",
      "Input: [1, 2, 3, 4, 5]\n",
      "Correct Output: [1, 1, 2, 1, 2, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5]\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise4(pickleFile, collectRange,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5b935f5d3971e9c5420a49a64e260b3",
     "grade": false,
     "grade_id": "12",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Consider the following RDDs: \n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "RDD1=sc.parallelize([\"spark basics\", \"big data analysis\", \"spring\"]) \n",
    "RDD2=sc.parallelize([\"spark using pyspark\", \"big data\"]) \n",
    "```\n",
    "\n",
    "Use set operations for the following exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7b4cc068e13f23a2d493e976751262ae",
     "grade": false,
     "grade_id": "13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercise 5\n",
    "\n",
    "Write a function **transform1(RDD1,RDD2)**\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "print transform1(RDD1,RDD2)\n",
    "print transform1(RDD1,RDD2).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "\n",
    "\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "['spark', 'basics', 'big', 'data', 'analysis', 'spring', 'spark', 'using', 'pyspark', 'big', 'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def transform1(RDD1,RDD2):\n",
    "    RDD1 = RDD1.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD2 = RDD2.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD = RDD1.union(RDD2)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da9abeb941e4eeaa6ea95f91bea2316f",
     "grade": true,
     "grade_id": "ex_5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['spark basics', 'big data analysis', 'spring']\n",
      "['spark using pyspark', 'big data']\n",
      "Correct Output: ['spark', 'basics', 'big', 'data', 'analysis', 'spring', 'spark', 'using', 'pyspark', 'big', 'data']\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise5(pickleFile, transform1,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "72127562d838b1387960341ecd6d3dbc",
     "grade": false,
     "grade_id": "14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercise 6\n",
    "\n",
    "Write a function **transform2(RDD1,RDD2)**\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "print transform2(RDD1,RDD2)\n",
    "print transform2(RDD1,RDD2).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "['data', 'big', 'spark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def transform2(RDD1,RDD2):\n",
    "    RDD1 = RDD1.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD2 = RDD2.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD = RDD1.intersection(RDD2)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3735f1a9fcfd8681f34991ed9466b953",
     "grade": true,
     "grade_id": "ex_6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['spark basics', 'big data analysis', 'spring']\n",
      "['spark using pyspark', 'big data']\n",
      "Correct Output: ['big', 'spark', 'data']\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise6(pickleFile, transform2,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f9de1dd4827a694de8d3239523b4e9f",
     "grade": false,
     "grade_id": "15",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Exercise 7\n",
    "\n",
    "Write a function **transform3(RDD1,RDD2)**\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "print transform3(RDD1,RDD2)\n",
    "print transform3(RDD1,RDD2).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "['spring', 'analysis', 'basics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def transform3(RDD1,RDD2):\n",
    "    RDD1 = RDD1.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD2 = RDD2.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD = RDD1.subtract(RDD2)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a995f9f50d4cca863bc0e91e7f8cfa3e",
     "grade": true,
     "grade_id": "ex_7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['spark basics', 'big data analysis', 'spring']\n",
      "['spark using pyspark', 'big data']\n",
      "Correct Output: ['spring', 'basics', 'analysis']\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise7(pickleFile, transform3,sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "036eb54a2676c1b0b6ab34b6837be25e",
     "grade": false,
     "grade_id": "16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Exercise 8\n",
    "\n",
    "Write a function **transform4(RDD1,RDD2)**\n",
    "######  <span style=\"color:blue\">Code:</span>\n",
    "```python\n",
    "print transform4(RDD1,RDD2)\n",
    "print transform4(RDD1,RDD2).collect()\n",
    "```\n",
    "######  <span style=\"color:magenta\">Output:</span>\n",
    "\n",
    "PythonRDD[14] at RDD at PythonRDD.scala:48\n",
    "\n",
    "[('spark', 'spark'), ('spark', 'using'), ('spark', 'pyspark'), ('basics', 'spark'), ('basics', 'using'), ('basics', 'pyspark'), ('spark', 'big'), ('spark', 'data'), ('basics', 'big'), ('basics', 'data'), ('big', 'spark'), ('big', 'using'), ('big', 'pyspark'), ('data', 'spark'), ('analysis', 'spark'), ('data', 'using'), ('data', 'pyspark'), ('analysis', 'using'), ('analysis', 'pyspark'), ('spring', 'spark'), ('spring', 'using'), ('spring', 'pyspark'), ('big', 'big'), ('big', 'data'), ('data', 'big'), ('analysis', 'big'), ('data', 'data'), ('analysis', 'data'), ('spring', 'big'), ('spring', 'data')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Modify this cell\n",
    "def transform4(RDD1,RDD2):\n",
    "    RDD1 = RDD1.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD2 = RDD2.flatMap(lambda x: x.split(\" \"))\n",
    "    RDD = RDD1.cartesian(RDD2)\n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0593f55e79f25b0eba246e369f807f72",
     "grade": true,
     "grade_id": "ex_8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ['spark basics', 'big data analysis', 'spring']\n",
      "['spark using pyspark', 'big data']\n",
      "Correct Output: [('spark', 'spark'), ('spark', 'using'), ('spark', 'pyspark'), ('basics', 'spark'), ('basics', 'using'), ('basics', 'pyspark'), ('spark', 'big'), ('spark', 'data'), ('basics', 'big'), ('basics', 'data'), ('big', 'spark'), ('big', 'using'), ('big', 'pyspark'), ('data', 'spark'), ('analysis', 'spark'), ('data', 'using'), ('data', 'pyspark'), ('analysis', 'using'), ('analysis', 'pyspark'), ('big', 'big'), ('big', 'data'), ('data', 'big'), ('analysis', 'big'), ('data', 'data'), ('analysis', 'data'), ('spring', 'spark'), ('spring', 'using'), ('spring', 'pyspark'), ('spring', 'big'), ('spring', 'data')]\n",
      "Great Job!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import Tester.SparkBasics2 as SparkBasics2\n",
    "SparkBasics2.exercise8(pickleFile, transform4,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6227b2eeaa4d09b6425017cee507d80f",
     "grade": false,
     "grade_id": "17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
